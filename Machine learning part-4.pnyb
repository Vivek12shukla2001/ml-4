{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfdhQT94VIWRGpRiNJJZOM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"q12AT-rcWb6t"}},{"cell_type":"markdown","source":[],"metadata":{"id":"GGeQnyuyWc-d"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Ax33vOSNXYys"}},{"cell_type":"markdown","source":["### SVM & Naïve Bayes – Theoretical and Practical (Pwskills Java + DSA)\n","\n","---\n","\n","## ✅ Theoretical Questions with Answers (Numbered)\n","\n","**Q1. What is a Support Vector Machine (SVM)?**\n","A supervised learning algorithm that finds the optimal hyperplane to classify data by maximizing the margin between different classes.\n","\n","**Q2. What is the difference between Hard Margin and Soft Margin SVM?**\n","\n","* **Hard Margin**: Assumes data is linearly separable with no errors. No misclassification allowed.\n","* **Soft Margin**: Allows some misclassification to improve generalization on noisy data. More realistic.\n","\n","**Q3. What is the mathematical intuition behind SVM?**\n","SVM aims to maximize the margin between classes by minimizing $\\frac{1}{2} ||w||^2$ subject to $y_i(w \\cdot x_i + b) \\geq 1$. The margin is the distance between support vectors and the hyperplane.\n","\n","**Q4. What is the role of Lagrange Multipliers in SVM?**\n","They help convert the constrained optimization problem into its dual form, making it easier to solve and apply the kernel trick.\n","\n","**Q5. What are Support Vectors in SVM?**\n","Support vectors are the data points closest to the separating hyperplane. They define the decision boundary.\n","\n","**Q6. What is a Support Vector Classifier (SVC)?**\n","SVC is an SVM implementation used for classification problems. It finds the hyperplane that best separates the classes.\n","\n","**Q7. What is a Support Vector Regressor (SVR)?**\n","SVR is a regression version of SVM. It fits a line (or curve) within a margin of tolerance and penalizes predictions outside the margin.\n","\n","**Q8. What is the Kernel Trick in SVM?**\n","The kernel trick transforms data into higher-dimensional space using functions (like polynomial, RBF) to enable separation by a hyperplane.\n","\n","**Q9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:**\n","\n","* **Linear**: Best for linearly separable data.\n","* **Polynomial**: Models complex relationships using polynomial degrees.\n","* **RBF**: Most flexible; handles nonlinear data by creating a Gaussian similarity-based decision surface.\n","\n","**Q10. What is the effect of the C parameter in SVM?**\n","\n","* High C: Low bias, high variance (less tolerance for misclassification)\n","* Low C: High bias, low variance (more regularized)\n","\n","**Q11. What is the role of the Gamma parameter in RBF Kernel SVM?**\n","Gamma defines how far the influence of a single training point reaches. Low gamma = far; high gamma = close, overfits.\n","\n","**Q12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n","A probabilistic classifier based on Bayes' Theorem assuming feature independence. \"Naïve\" because it assumes features are conditionally independent given the class.\n","\n","**Q13. What is Bayes’ Theorem?**\n","$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n","\n","**Q14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes:**\n","\n","* **Gaussian**: Assumes features follow a normal distribution.\n","* **Multinomial**: Works with discrete count features (e.g., word counts).\n","* **Bernoulli**: For binary features.\n","\n","**Q15. When should you use Gaussian Naïve Bayes over other variants?**\n","When your features are continuous and roughly normally distributed.\n","\n","**Q16. What are the key assumptions made by Naïve Bayes?**\n","\n","* Conditional independence of features.\n","* Each feature contributes independently to the outcome.\n","\n","**Q17. What are the advantages and disadvantages of Naïve Bayes?**\n","**Advantages**: Simple, fast, works well with high dimensions and text data.\n","**Disadvantages**: Assumes independence, which may not hold true; sensitive to correlated features.\n","\n","**Q18. Why is Naïve Bayes a good choice for text classification?**\n","Words are treated independently (bag-of-words model), aligning with the independence assumption of Naïve Bayes.\n","\n","**Q19. Compare SVM and Naïve Bayes for classification tasks:**\n","\n","* **SVM**: Better for complex, high-dimensional data with clear margins.\n","* **Naïve Bayes**: Works well with categorical or text data, fast and interpretable.\n","\n","**Q20. How does Laplace Smoothing help in Naïve Bayes?**\n","It prevents zero probabilities for unseen features by adding 1 to every count, improving generalization.\n","\n","---\n","\n","\n"],"metadata":{"id":"SMFg-tbfX5qp"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Cc99KFnAX5tG"}},{"cell_type":"markdown","source":[],"metadata":{"id":"AhVCaGzQX5vv"}},{"cell_type":"markdown","source":[],"metadata":{"id":"-knPR9BdX5zG"}}]}